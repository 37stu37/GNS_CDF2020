# -*- coding: utf-8 -*-
"""Training_set_tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_x5yW0G0itBmY101ibrm8aPUmAwLhrkz
"""

# Commented out IPython magic to ensure Python compatibility.
# Load the Drive helper and mount
from google.colab import drive
# %tensorflow_version 2.x
# This will prompt for authorization.
drive.mount('/content/drive')

import os
import pandas as pd
import numpy as np
import glob
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split

from tensorflow import keras

"""Set the dataset"""

# path and data parallel computing akatea
path = '/content/drive/My Drive/04_Cloud/01_Work/GNS/CDF_2019/4Rob'
csv_input_edge_list = (os.path.join(path, 'input_edgeList.csv'))
path_data = (os.path.join(path, 'output_algorithm_data_sample'))
print(path_data)

## compile and merge all available output into a training dataframe 
def TrainingSet(network_edge_list, sample_size):
    chunk_list = []
    files = glob.glob(os.path.join(path_data, "*.csv"))
    for file in files:
        print(os.path.join(path_data, file))
        chunks = pd.read_csv(os.path.join(path_data, file), header=0, chunksize=100000, low_memory=False)
        for chunk in chunks:
                chunk = pd.DataFrame(chunk, columns=['ID_unique','ID_unique_haz','event_magnitude'])#.sample(s)
                training_chunk = chunk.merge(network_edge_list, left_on=['ID_unique', 'ID_unique_haz'], right_on=['ID_unique', 'ID_unique_haz'], how='inner', suffixes=('_OUT','_IN'))
                chunk_list.append(training_chunk)
    training_set = pd.concat(chunk_list)
    training_set = training_set.sample(sample_size)
    return training_set
    # training_set.to_csv(os.path.join(path, 'CDF', "training_set_sample_{}.csv".format(s)))

df_input = pd.read_csv(csv_input_edge_list, usecols = ['ID_unique','EQ2y','EQ5y','EQ10y','EQ25y','EQ50y',
           'EQ100y','EQ200y','EQ500y','EQ1000y','EQ5000y','EQ10000y',
           'fdem25fj','FL1700','FL2000','FL2300','FL2500','FL2900','SB_FL1700','SB_FL2000','SB_FL2300','SB_FL2500','SB_FL2900',
           'LSeq','LSrf','RF10y','RF20y','RF50y','RF100y','AREA','W_area',
           'ID_unique_haz','distance','angle', 
           'SLP_mean','SLP_mean_haz'])

df = TrainingSet(df_input, 200000)
len(df)
df.sort_values(by=['ID_unique', 'ID_unique_haz'], inplace=True)
df.describe()

from sklearn import preprocessing
# create the Labelencoder object
le = preprocessing.LabelEncoder()
#convert the categorical columns into numeric
df['ID_unique'] = le.fit_transform(df['ID_unique'])
df['ID_unique_haz'] = le.fit_transform(df['ID_unique_haz'])
min_max_scaler = preprocessing.MinMaxScaler()
d_scaled = min_max_scaler.fit_transform(df)
data = pd.DataFrame(d_scaled, columns=df.columns)
# data.info() 
data.describe()

train_dataset = data.sample(frac=0.8,random_state=0)
test_dataset = data.drop(train_dataset.index)

# sns.pairplot(train_dataset[["col1", "col2"]], diag_kind="kde")

train_stats = train_dataset.describe()
train_stats = train_stats.transpose()
train_stats

"""Once train dataset is done - model set up"""

from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow as tf

tf.keras.backend.clear_session()  # For easy reset of notebook state.

from tensorflow import keras
from tensorflow.keras import layers

inputs = keras.Input(shape=(784,), name='digits')
x = layers.Dense(64, activation='relu', name='dense_1')(inputs)
x = layers.Dense(64, activation='relu', name='dense_2')(x)
outputs = layers.Dense(10, activation='softmax', name='predictions')(x)

model = keras.Model(inputs=inputs, outputs=outputs)

# Load a toy dataset for the sake of this example
# (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Preprocess the data (these are Numpy arrays)
x_train = x_train.reshape(60000, 784).astype('float32') / 255
x_test = x_test.reshape(10000, 784).astype('float32') / 255

y_train = y_train.astype('float32')
y_test = y_test.astype('float32')

# Reserve 10,000 samples for validation
x_val = x_train[-10000:]
y_val = y_train[-10000:]
x_train = x_train[:-10000]
y_train = y_train[:-10000]

# Specify the training configuration (optimizer, loss, metrics)
model.compile(optimizer=keras.optimizers.RMSprop(),  # Optimizer
              # Loss function to minimize
              loss=keras.losses.SparseCategoricalCrossentropy(),
              # List of metrics to monitor
              metrics=[keras.metrics.SparseCategoricalAccuracy()])

# Train the model by slicing the data into "batches"
# of size "batch_size", and repeatedly iterating over
# the entire dataset for a given number of "epochs"
print('# Fit model on training data')
history = model.fit(x_train, y_train,
                    batch_size=64,
                    epochs=3,
                    # We pass some validation for
                    # monitoring validation loss and metrics
                    # at the end of each epoch
                    validation_data=(x_val, y_val))

# The returned "history" object holds a record
# of the loss values and metric values during training
print('\nhistory dict:', history.history)

# Evaluate the model on the test data using `evaluate`
print('\n# Evaluate on test data')
results = model.evaluate(x_test, y_test, batch_size=128)
print('test loss, test acc:', results)

# Generate predictions (probabilities -- the output of the last layer)
# on new data using `predict`
print('\n# Generate predictions for 3 samples')
predictions = model.predict(x_test[:3])
print('predictions shape:', predictions.shape)

"""###Specifying a loss, metrics, and an optimizer

To train a model with fit, you need to specify a loss function, an optimizer, and optionally, some metrics to monitor.

You pass these to the model as arguments to the compile() method:
"""

model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),
              loss=keras.losses.SparseCategoricalCrossentropy(),
              metrics=[keras.metrics.SparseCategoricalAccuracy()])

"""The metrics argument should be a list -- your model can have any number of metrics.

If your model has multiple outputs, you can specify different losses and metrics for each output, and you can modulate the contribution of each output to the total loss of the model. You will find more details about this in the section "Passing data to multi-input, multi-output models".

Note that in many cases, the loss and metrics are specified via string identifiers, as a shortcut:
"""

model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),
              loss='sparse_categorical_crossentropy',
              metrics=['sparse_categorical_accuracy'])

"""let's put our model definition and compile step in functions; we will call them several times across different examples in this guide"""

def get_uncompiled_model():
  inputs = keras.Input(shape=(784,), name='digits')
  x = layers.Dense(64, activation='relu', name='dense_1')(inputs)
  x = layers.Dense(64, activation='relu', name='dense_2')(x)
  outputs = layers.Dense(10, activation='softmax', name='predictions')(x)
  model = keras.Model(inputs=inputs, outputs=outputs)
  return model

def get_compiled_model():
  model = get_uncompiled_model()
  model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),
              loss='sparse_categorical_crossentropy',
              metrics=['sparse_categorical_accuracy'])
  return model

"""Many built-in optimizers, losses, and metrics are available
In general, you won't have to create from scratch your own losses, metrics, or optimizers, because what you need is likely already part of the Keras API:

Optimizers:

SGD() (with or without momentum)
RMSprop()
Adam()
etc.

Losses:

MeanSquaredError()
KLDivergence()
CosineSimilarity()
etc.
Metrics:

AUC()
Precision()
Recall()
etc.

#Custom losses

There are two ways to provide custom losses with Keras. The first example creates a function that accepts inputs y_true and y_pred. The following example shows a loss function that computes the average distance between the real data and the predictions:
"""

def basic_loss_function(y_true, y_pred):
    return tf.math.reduce_mean(y_true - y_pred)

model.compile(optimizer=keras.optimizers.Adam(),
              loss=basic_loss_function)

model.fit(x_train, y_train, batch_size=64, epochs=3)